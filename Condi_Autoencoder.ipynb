{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLZdFpaYax9h"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "y14nCJEya4El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "YGTAuiAsa4HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "406yhYd3a4Jm",
        "outputId": "a4949b4b-8835-42ad-94da-df5952abd436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################Change Data list #######################################\n",
        "data=pd.read_csv('/content/drive/MyDrive/AICP 연습/1020_5y_data_FINISH.csv')\n",
        "print(data.shape)\n",
        "print()\n",
        "data=data.dropna(axis=0)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA3FLhx8a4L0",
        "outputId": "bdc4c7b1-8276-432e-9767-48b12799243f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11132, 17)\n",
            "\n",
            "(11132, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "66viSmcubAg1",
        "outputId": "1146ea7f-ac90-4b68-dcdc-a61c336d8ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Pcr  Stock       Name         종가         시가         고가         저가  \\\n",
              "0    1471  33780   (주)케이티앤지    96300.0   100500.0   100500.0    96300.0   \n",
              "1   64847  32640  (주)LG유플러스    16800.0    17250.0    17250.0    16500.0   \n",
              "2   86512  69960   (주)현대백화점    91000.0    88500.0    91800.0    88500.0   \n",
              "3   88102  20000      (주)한섬    34650.0    35400.0    36350.0    34600.0   \n",
              "4  162210  51900  (주)LG생활건강  1112000.0  1048000.0  1125000.0  1047000.0   \n",
              "\n",
              "        대용가        거래량          거래대금    PER   PSR   PBR    PCR  P/EBITDAPS  \\\n",
              "0   79520.0   745547.0  7.296529e+10  12.47  4.05  1.86  11.03        9.03   \n",
              "1   13140.0  1844022.0  3.116109e+10  12.86  0.60  1.69   3.22        2.90   \n",
              "2   68710.0    89791.0  8.145079e+09   1.02  0.15  0.06   0.70        0.57   \n",
              "3   25300.0   106143.0  3.739379e+09  13.95  0.99  0.93  17.05        8.38   \n",
              "4  816660.0    67214.0  7.368183e+10  31.29  4.92  8.92  25.11       24.63   \n",
              "\n",
              "   Dividend Yield  Volatility  \n",
              "0            3.82       21.57  \n",
              "1            2.38       33.35  \n",
              "2            8.47       30.84  \n",
              "3            0.90       36.97  \n",
              "4            0.76       35.95  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c536ca7c-309d-4ceb-b0b3-0388cccd457f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pcr</th>\n",
              "      <th>Stock</th>\n",
              "      <th>Name</th>\n",
              "      <th>종가</th>\n",
              "      <th>시가</th>\n",
              "      <th>고가</th>\n",
              "      <th>저가</th>\n",
              "      <th>대용가</th>\n",
              "      <th>거래량</th>\n",
              "      <th>거래대금</th>\n",
              "      <th>PER</th>\n",
              "      <th>PSR</th>\n",
              "      <th>PBR</th>\n",
              "      <th>PCR</th>\n",
              "      <th>P/EBITDAPS</th>\n",
              "      <th>Dividend Yield</th>\n",
              "      <th>Volatility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1471</td>\n",
              "      <td>33780</td>\n",
              "      <td>(주)케이티앤지</td>\n",
              "      <td>96300.0</td>\n",
              "      <td>100500.0</td>\n",
              "      <td>100500.0</td>\n",
              "      <td>96300.0</td>\n",
              "      <td>79520.0</td>\n",
              "      <td>745547.0</td>\n",
              "      <td>7.296529e+10</td>\n",
              "      <td>12.47</td>\n",
              "      <td>4.05</td>\n",
              "      <td>1.86</td>\n",
              "      <td>11.03</td>\n",
              "      <td>9.03</td>\n",
              "      <td>3.82</td>\n",
              "      <td>21.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64847</td>\n",
              "      <td>32640</td>\n",
              "      <td>(주)LG유플러스</td>\n",
              "      <td>16800.0</td>\n",
              "      <td>17250.0</td>\n",
              "      <td>17250.0</td>\n",
              "      <td>16500.0</td>\n",
              "      <td>13140.0</td>\n",
              "      <td>1844022.0</td>\n",
              "      <td>3.116109e+10</td>\n",
              "      <td>12.86</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.69</td>\n",
              "      <td>3.22</td>\n",
              "      <td>2.90</td>\n",
              "      <td>2.38</td>\n",
              "      <td>33.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86512</td>\n",
              "      <td>69960</td>\n",
              "      <td>(주)현대백화점</td>\n",
              "      <td>91000.0</td>\n",
              "      <td>88500.0</td>\n",
              "      <td>91800.0</td>\n",
              "      <td>88500.0</td>\n",
              "      <td>68710.0</td>\n",
              "      <td>89791.0</td>\n",
              "      <td>8.145079e+09</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.57</td>\n",
              "      <td>8.47</td>\n",
              "      <td>30.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>88102</td>\n",
              "      <td>20000</td>\n",
              "      <td>(주)한섬</td>\n",
              "      <td>34650.0</td>\n",
              "      <td>35400.0</td>\n",
              "      <td>36350.0</td>\n",
              "      <td>34600.0</td>\n",
              "      <td>25300.0</td>\n",
              "      <td>106143.0</td>\n",
              "      <td>3.739379e+09</td>\n",
              "      <td>13.95</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.93</td>\n",
              "      <td>17.05</td>\n",
              "      <td>8.38</td>\n",
              "      <td>0.90</td>\n",
              "      <td>36.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>162210</td>\n",
              "      <td>51900</td>\n",
              "      <td>(주)LG생활건강</td>\n",
              "      <td>1112000.0</td>\n",
              "      <td>1048000.0</td>\n",
              "      <td>1125000.0</td>\n",
              "      <td>1047000.0</td>\n",
              "      <td>816660.0</td>\n",
              "      <td>67214.0</td>\n",
              "      <td>7.368183e+10</td>\n",
              "      <td>31.29</td>\n",
              "      <td>4.92</td>\n",
              "      <td>8.92</td>\n",
              "      <td>25.11</td>\n",
              "      <td>24.63</td>\n",
              "      <td>0.76</td>\n",
              "      <td>35.95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c536ca7c-309d-4ceb-b0b3-0388cccd457f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c536ca7c-309d-4ceb-b0b3-0388cccd457f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c536ca7c-309d-4ceb-b0b3-0388cccd457f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1e3a5331-6a7e-4173-9b32-2e1d561e6ece\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e3a5331-6a7e-4173-9b32-2e1d561e6ece')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1e3a5331-6a7e-4173-9b32-2e1d561e6ece button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_df(df):\n",
        "\n",
        "    for col in list(df.columns):\n",
        "\n",
        "        mean, std = df[col].mean(), df[col].std()\n",
        "\n",
        "        df.loc[:, col] = (df[col] -mean) /(std + 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "a=list(data)"
      ],
      "metadata": {
        "id": "89ybT1lIbM3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeuwdEo4bM9N",
        "outputId": "abb1a1c0-cac0-4ef9-a687-bc0f4dbe3822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11132 entries, 0 to 11131\n",
            "Data columns (total 17 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Pcr             11132 non-null  object \n",
            " 1   Stock           11132 non-null  int64  \n",
            " 2   Name            11132 non-null  object \n",
            " 3   종가              11132 non-null  float64\n",
            " 4   시가              11132 non-null  float64\n",
            " 5   고가              11132 non-null  float64\n",
            " 6   저가              11132 non-null  float64\n",
            " 7   대용가             11132 non-null  float64\n",
            " 8   거래량             11132 non-null  float64\n",
            " 9   거래대금            11132 non-null  float64\n",
            " 10  PER             11132 non-null  float64\n",
            " 11  PSR             11132 non-null  float64\n",
            " 12  PBR             11132 non-null  float64\n",
            " 13  PCR             11132 non-null  float64\n",
            " 14  P/EBITDAPS      11132 non-null  float64\n",
            " 15  Dividend Yield  11132 non-null  float64\n",
            " 16  Volatility      11132 non-null  float64\n",
            "dtypes: float64(14), int64(1), object(2)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = normalize_df(data.iloc[:, 3:])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4sq9VKwxbNAF",
        "outputId": "64d89b36-b0ac-4a7f-e194-221ae551c617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         종가        시가        고가        저가       대용가       거래량      거래대금  \\\n",
              "0 -0.269104 -0.248904 -0.256406 -0.262060 -0.231692  0.002092  0.069545   \n",
              "1 -0.646417 -0.643420 -0.646194 -0.645382 -0.635234  0.401617 -0.143055   \n",
              "2 -0.294258 -0.305771 -0.297141 -0.299527 -0.297409 -0.236412 -0.260105   \n",
              "3 -0.561699 -0.557408 -0.556765 -0.558438 -0.561310 -0.230464 -0.282511   \n",
              "4  4.551482  4.241237  4.540433  4.304659  4.249587 -0.244623  0.073189   \n",
              "\n",
              "        PER       PSR       PBR       PCR  P/EBITDAPS  Dividend Yield  \\\n",
              "0 -0.434879  0.266372 -0.119227 -0.076769   -0.296589        0.767661   \n",
              "1 -0.421849 -0.364780 -0.168723 -0.111431   -0.677175        0.164256   \n",
              "2 -0.817455 -0.447104 -0.643311 -0.122615   -0.821835        2.716157   \n",
              "3 -0.385429 -0.293433 -0.390004 -0.050052   -0.336944       -0.455911   \n",
              "4  0.193947  0.425532  1.936350 -0.014280    0.671951       -0.514575   \n",
              "\n",
              "   Volatility  \n",
              "0   -1.139185  \n",
              "1   -0.143984  \n",
              "2   -0.356035  \n",
              "3    0.161841  \n",
              "4    0.075669  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62ea72c7-4c77-4dae-94d7-bdff531a96a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>종가</th>\n",
              "      <th>시가</th>\n",
              "      <th>고가</th>\n",
              "      <th>저가</th>\n",
              "      <th>대용가</th>\n",
              "      <th>거래량</th>\n",
              "      <th>거래대금</th>\n",
              "      <th>PER</th>\n",
              "      <th>PSR</th>\n",
              "      <th>PBR</th>\n",
              "      <th>PCR</th>\n",
              "      <th>P/EBITDAPS</th>\n",
              "      <th>Dividend Yield</th>\n",
              "      <th>Volatility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.269104</td>\n",
              "      <td>-0.248904</td>\n",
              "      <td>-0.256406</td>\n",
              "      <td>-0.262060</td>\n",
              "      <td>-0.231692</td>\n",
              "      <td>0.002092</td>\n",
              "      <td>0.069545</td>\n",
              "      <td>-0.434879</td>\n",
              "      <td>0.266372</td>\n",
              "      <td>-0.119227</td>\n",
              "      <td>-0.076769</td>\n",
              "      <td>-0.296589</td>\n",
              "      <td>0.767661</td>\n",
              "      <td>-1.139185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.646417</td>\n",
              "      <td>-0.643420</td>\n",
              "      <td>-0.646194</td>\n",
              "      <td>-0.645382</td>\n",
              "      <td>-0.635234</td>\n",
              "      <td>0.401617</td>\n",
              "      <td>-0.143055</td>\n",
              "      <td>-0.421849</td>\n",
              "      <td>-0.364780</td>\n",
              "      <td>-0.168723</td>\n",
              "      <td>-0.111431</td>\n",
              "      <td>-0.677175</td>\n",
              "      <td>0.164256</td>\n",
              "      <td>-0.143984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.294258</td>\n",
              "      <td>-0.305771</td>\n",
              "      <td>-0.297141</td>\n",
              "      <td>-0.299527</td>\n",
              "      <td>-0.297409</td>\n",
              "      <td>-0.236412</td>\n",
              "      <td>-0.260105</td>\n",
              "      <td>-0.817455</td>\n",
              "      <td>-0.447104</td>\n",
              "      <td>-0.643311</td>\n",
              "      <td>-0.122615</td>\n",
              "      <td>-0.821835</td>\n",
              "      <td>2.716157</td>\n",
              "      <td>-0.356035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.561699</td>\n",
              "      <td>-0.557408</td>\n",
              "      <td>-0.556765</td>\n",
              "      <td>-0.558438</td>\n",
              "      <td>-0.561310</td>\n",
              "      <td>-0.230464</td>\n",
              "      <td>-0.282511</td>\n",
              "      <td>-0.385429</td>\n",
              "      <td>-0.293433</td>\n",
              "      <td>-0.390004</td>\n",
              "      <td>-0.050052</td>\n",
              "      <td>-0.336944</td>\n",
              "      <td>-0.455911</td>\n",
              "      <td>0.161841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.551482</td>\n",
              "      <td>4.241237</td>\n",
              "      <td>4.540433</td>\n",
              "      <td>4.304659</td>\n",
              "      <td>4.249587</td>\n",
              "      <td>-0.244623</td>\n",
              "      <td>0.073189</td>\n",
              "      <td>0.193947</td>\n",
              "      <td>0.425532</td>\n",
              "      <td>1.936350</td>\n",
              "      <td>-0.014280</td>\n",
              "      <td>0.671951</td>\n",
              "      <td>-0.514575</td>\n",
              "      <td>0.075669</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62ea72c7-4c77-4dae-94d7-bdff531a96a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62ea72c7-4c77-4dae-94d7-bdff531a96a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62ea72c7-4c77-4dae-94d7-bdff531a96a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac0906fe-0809-423b-9fed-6f62762ed454\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac0906fe-0809-423b-9fed-6f62762ed454')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac0906fe-0809-423b-9fed-6f62762ed454 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsBque_vbNCa",
        "outputId": "70023d1c-38cb-404e-c16c-126ec5275b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11132, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(drop=True, inplace=True)\n",
        "df=pd.concat([data.iloc[:,:3], df], axis=1)\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "print(df.shape)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "HogxEQj4bNEy",
        "outputId": "9c8194bd-5c15-4afc-869c-6f2c0cad8721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11132, 17)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Pcr   Stock            Name        종가        시가        고가        저가  \\\n",
              "0        1471   33780        (주)케이티앤지 -0.269104 -0.248904 -0.256406 -0.262060   \n",
              "1       64847   32640       (주)LG유플러스 -0.646417 -0.643420 -0.646194 -0.645382   \n",
              "2       86512   69960        (주)현대백화점 -0.294258 -0.305771 -0.297141 -0.299527   \n",
              "3       88102   20000           (주)한섬 -0.561699 -0.557408 -0.556765 -0.558438   \n",
              "4      162210   51900       (주)LG생활건강  4.551482  4.241237  4.540433  4.304659   \n",
              "...       ...     ...             ...       ...       ...       ...       ...   \n",
              "11127  H32072  139480          (주)이마트 -0.389654 -0.386333 -0.388442 -0.387912   \n",
              "11128  I66472  161390  한국타이어앤테크놀로지(주) -0.549834 -0.544613 -0.548337 -0.546909   \n",
              "11129  IQ7228  271560          (주)오리온 -0.174657 -0.165972 -0.168851 -0.169832   \n",
              "11130  IV0057  280360        롯데웰푸드(주) -0.208829 -0.199619 -0.207712 -0.210662   \n",
              "11131  L07656  185750          (주)종근당 -0.286664 -0.288711 -0.286372 -0.288960   \n",
              "\n",
              "            대용가       거래량      거래대금       PER       PSR       PBR       PCR  \\\n",
              "0     -0.231692  0.002092  0.069545 -0.434879  0.266372 -0.119227 -0.076769   \n",
              "1     -0.635234  0.401617 -0.143055 -0.421849 -0.364780 -0.168723 -0.111431   \n",
              "2     -0.297409 -0.236412 -0.260105 -0.817455 -0.447104 -0.643311 -0.122615   \n",
              "3     -0.561310 -0.230464 -0.282511 -0.385429 -0.293433 -0.390004 -0.050052   \n",
              "4      4.249587 -0.244623  0.073189  0.193947  0.425532  1.936350 -0.014280   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "11127 -0.391698 -0.244402 -0.277019 -0.789722 -0.452593 -0.605461 -0.103132   \n",
              "11128 -0.541188 -0.205763 -0.268497 -0.276503 -0.234891 -0.387092 -0.051206   \n",
              "11129 -0.149925 -0.245506 -0.263085  0.614613  0.420044  0.797921  0.026684   \n",
              "11130 -0.206463 -0.267089 -0.298499  0.634994 -0.412345 -0.509378 -0.061458   \n",
              "11131 -0.284095 -0.251458 -0.278815 -0.416503 -0.340998 -0.122138 -0.050762   \n",
              "\n",
              "       P/EBITDAPS  Dividend Yield  Volatility  \n",
              "0       -0.296589        0.767661   -1.139185  \n",
              "1       -0.677175        0.164256   -0.143984  \n",
              "2       -0.821835        2.716157   -0.356035  \n",
              "3       -0.336944       -0.455911    0.161841  \n",
              "4        0.671951       -0.514575    0.075669  \n",
              "...           ...             ...         ...  \n",
              "11127   -0.706976        0.302536   -0.428689  \n",
              "11128    1.208373        0.055307   -0.205656  \n",
              "11129    0.675676       -0.489433   -0.373776  \n",
              "11130   -0.570387       -0.003357   -1.235494  \n",
              "11131   -0.371092       -0.393056   -0.787739  \n",
              "\n",
              "[11132 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79732feb-789c-4bce-b3f0-5471eee74935\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pcr</th>\n",
              "      <th>Stock</th>\n",
              "      <th>Name</th>\n",
              "      <th>종가</th>\n",
              "      <th>시가</th>\n",
              "      <th>고가</th>\n",
              "      <th>저가</th>\n",
              "      <th>대용가</th>\n",
              "      <th>거래량</th>\n",
              "      <th>거래대금</th>\n",
              "      <th>PER</th>\n",
              "      <th>PSR</th>\n",
              "      <th>PBR</th>\n",
              "      <th>PCR</th>\n",
              "      <th>P/EBITDAPS</th>\n",
              "      <th>Dividend Yield</th>\n",
              "      <th>Volatility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1471</td>\n",
              "      <td>33780</td>\n",
              "      <td>(주)케이티앤지</td>\n",
              "      <td>-0.269104</td>\n",
              "      <td>-0.248904</td>\n",
              "      <td>-0.256406</td>\n",
              "      <td>-0.262060</td>\n",
              "      <td>-0.231692</td>\n",
              "      <td>0.002092</td>\n",
              "      <td>0.069545</td>\n",
              "      <td>-0.434879</td>\n",
              "      <td>0.266372</td>\n",
              "      <td>-0.119227</td>\n",
              "      <td>-0.076769</td>\n",
              "      <td>-0.296589</td>\n",
              "      <td>0.767661</td>\n",
              "      <td>-1.139185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64847</td>\n",
              "      <td>32640</td>\n",
              "      <td>(주)LG유플러스</td>\n",
              "      <td>-0.646417</td>\n",
              "      <td>-0.643420</td>\n",
              "      <td>-0.646194</td>\n",
              "      <td>-0.645382</td>\n",
              "      <td>-0.635234</td>\n",
              "      <td>0.401617</td>\n",
              "      <td>-0.143055</td>\n",
              "      <td>-0.421849</td>\n",
              "      <td>-0.364780</td>\n",
              "      <td>-0.168723</td>\n",
              "      <td>-0.111431</td>\n",
              "      <td>-0.677175</td>\n",
              "      <td>0.164256</td>\n",
              "      <td>-0.143984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86512</td>\n",
              "      <td>69960</td>\n",
              "      <td>(주)현대백화점</td>\n",
              "      <td>-0.294258</td>\n",
              "      <td>-0.305771</td>\n",
              "      <td>-0.297141</td>\n",
              "      <td>-0.299527</td>\n",
              "      <td>-0.297409</td>\n",
              "      <td>-0.236412</td>\n",
              "      <td>-0.260105</td>\n",
              "      <td>-0.817455</td>\n",
              "      <td>-0.447104</td>\n",
              "      <td>-0.643311</td>\n",
              "      <td>-0.122615</td>\n",
              "      <td>-0.821835</td>\n",
              "      <td>2.716157</td>\n",
              "      <td>-0.356035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>88102</td>\n",
              "      <td>20000</td>\n",
              "      <td>(주)한섬</td>\n",
              "      <td>-0.561699</td>\n",
              "      <td>-0.557408</td>\n",
              "      <td>-0.556765</td>\n",
              "      <td>-0.558438</td>\n",
              "      <td>-0.561310</td>\n",
              "      <td>-0.230464</td>\n",
              "      <td>-0.282511</td>\n",
              "      <td>-0.385429</td>\n",
              "      <td>-0.293433</td>\n",
              "      <td>-0.390004</td>\n",
              "      <td>-0.050052</td>\n",
              "      <td>-0.336944</td>\n",
              "      <td>-0.455911</td>\n",
              "      <td>0.161841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>162210</td>\n",
              "      <td>51900</td>\n",
              "      <td>(주)LG생활건강</td>\n",
              "      <td>4.551482</td>\n",
              "      <td>4.241237</td>\n",
              "      <td>4.540433</td>\n",
              "      <td>4.304659</td>\n",
              "      <td>4.249587</td>\n",
              "      <td>-0.244623</td>\n",
              "      <td>0.073189</td>\n",
              "      <td>0.193947</td>\n",
              "      <td>0.425532</td>\n",
              "      <td>1.936350</td>\n",
              "      <td>-0.014280</td>\n",
              "      <td>0.671951</td>\n",
              "      <td>-0.514575</td>\n",
              "      <td>0.075669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11127</th>\n",
              "      <td>H32072</td>\n",
              "      <td>139480</td>\n",
              "      <td>(주)이마트</td>\n",
              "      <td>-0.389654</td>\n",
              "      <td>-0.386333</td>\n",
              "      <td>-0.388442</td>\n",
              "      <td>-0.387912</td>\n",
              "      <td>-0.391698</td>\n",
              "      <td>-0.244402</td>\n",
              "      <td>-0.277019</td>\n",
              "      <td>-0.789722</td>\n",
              "      <td>-0.452593</td>\n",
              "      <td>-0.605461</td>\n",
              "      <td>-0.103132</td>\n",
              "      <td>-0.706976</td>\n",
              "      <td>0.302536</td>\n",
              "      <td>-0.428689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11128</th>\n",
              "      <td>I66472</td>\n",
              "      <td>161390</td>\n",
              "      <td>한국타이어앤테크놀로지(주)</td>\n",
              "      <td>-0.549834</td>\n",
              "      <td>-0.544613</td>\n",
              "      <td>-0.548337</td>\n",
              "      <td>-0.546909</td>\n",
              "      <td>-0.541188</td>\n",
              "      <td>-0.205763</td>\n",
              "      <td>-0.268497</td>\n",
              "      <td>-0.276503</td>\n",
              "      <td>-0.234891</td>\n",
              "      <td>-0.387092</td>\n",
              "      <td>-0.051206</td>\n",
              "      <td>1.208373</td>\n",
              "      <td>0.055307</td>\n",
              "      <td>-0.205656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11129</th>\n",
              "      <td>IQ7228</td>\n",
              "      <td>271560</td>\n",
              "      <td>(주)오리온</td>\n",
              "      <td>-0.174657</td>\n",
              "      <td>-0.165972</td>\n",
              "      <td>-0.168851</td>\n",
              "      <td>-0.169832</td>\n",
              "      <td>-0.149925</td>\n",
              "      <td>-0.245506</td>\n",
              "      <td>-0.263085</td>\n",
              "      <td>0.614613</td>\n",
              "      <td>0.420044</td>\n",
              "      <td>0.797921</td>\n",
              "      <td>0.026684</td>\n",
              "      <td>0.675676</td>\n",
              "      <td>-0.489433</td>\n",
              "      <td>-0.373776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11130</th>\n",
              "      <td>IV0057</td>\n",
              "      <td>280360</td>\n",
              "      <td>롯데웰푸드(주)</td>\n",
              "      <td>-0.208829</td>\n",
              "      <td>-0.199619</td>\n",
              "      <td>-0.207712</td>\n",
              "      <td>-0.210662</td>\n",
              "      <td>-0.206463</td>\n",
              "      <td>-0.267089</td>\n",
              "      <td>-0.298499</td>\n",
              "      <td>0.634994</td>\n",
              "      <td>-0.412345</td>\n",
              "      <td>-0.509378</td>\n",
              "      <td>-0.061458</td>\n",
              "      <td>-0.570387</td>\n",
              "      <td>-0.003357</td>\n",
              "      <td>-1.235494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11131</th>\n",
              "      <td>L07656</td>\n",
              "      <td>185750</td>\n",
              "      <td>(주)종근당</td>\n",
              "      <td>-0.286664</td>\n",
              "      <td>-0.288711</td>\n",
              "      <td>-0.286372</td>\n",
              "      <td>-0.288960</td>\n",
              "      <td>-0.284095</td>\n",
              "      <td>-0.251458</td>\n",
              "      <td>-0.278815</td>\n",
              "      <td>-0.416503</td>\n",
              "      <td>-0.340998</td>\n",
              "      <td>-0.122138</td>\n",
              "      <td>-0.050762</td>\n",
              "      <td>-0.371092</td>\n",
              "      <td>-0.393056</td>\n",
              "      <td>-0.787739</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11132 rows × 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79732feb-789c-4bce-b3f0-5471eee74935')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79732feb-789c-4bce-b3f0-5471eee74935 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79732feb-789c-4bce-b3f0-5471eee74935');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-44911908-79ad-40f7-8225-cfff1f4bb158\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44911908-79ad-40f7-8225-cfff1f4bb158')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-44911908-79ad-40f7-8225-cfff1f4bb158 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Individual_Data"
      ],
      "metadata": {
        "id": "WQfYXwrwbJKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-zG4Vx9mEOk",
        "outputId": "c6b41faa-3c3f-42c4-86d4-1d94b796eb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=[df]\n",
        "a_name=['aa']"
      ],
      "metadata": {
        "id": "EBUU46wvz1ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "1KDbIXofbh4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  Device=torch.device('cuda')\n",
        "else:\n",
        "  Device=torch.device('cpu')\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "TbiVZfGOMSfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzc2KKi-02zG"
      },
      "outputs": [],
      "source": [
        "class Factors(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Factors, self).__init__()\n",
        "        #encoder\n",
        "        self.fc1 = nn.Linear(no_of_stocks, encoding_dim)\n",
        "\n",
        "        #decoder\n",
        "        self.fc2 = nn.Linear(encoding_dim, no_of_metrics)\n",
        "\n",
        "        #encoder\n",
        "        self.fc3 = nn.Linear(no_of_metrics_flat, encoding_dim)\n",
        "\n",
        "        #decoder\n",
        "        self.fc4 = nn.Linear(encoding_dim, no_of_metrics_flat)\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        x = (self.fc2(x))\n",
        "\n",
        "        y = torch.flatten(y)\n",
        "\n",
        "        y = F.relu(self.fc3(y))\n",
        "\n",
        "        y = F.relu(self.fc4(y))\n",
        "\n",
        "        y = y.view(no_of_metrics, no_of_stocks)\n",
        "        x = x.view(1,no_of_metrics)\n",
        "\n",
        "        w = torch.mm(x, y)\n",
        "\n",
        "        return w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start=time.time()"
      ],
      "metadata": {
        "id": "xbAwLCbBPFN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whole_data=pd.DataFrame()\n",
        "\n",
        "for j, name in zip(a, a_name):\n",
        "  # df_1=pd.DataFrame()\n",
        "  x_input = j.종가.to_numpy()\n",
        "  x = x_input.reshape(1, len(j.종가))\n",
        "  print(x.shape)\n",
        "\n",
        "  y = j.iloc[:,3:16].to_numpy()\n",
        "  print(y.shape)\n",
        "\n",
        "  print()\n",
        "\n",
        "# initialize the NN\n",
        "\n",
        "  no_of_stocks = len(j.종가)\n",
        "  no_of_metrics = 13\n",
        "  no_of_metrics_flat = no_of_stocks * no_of_metrics  #metrics.shape[1]\n",
        "  encoding_dim = 32\n",
        "\n",
        "  model = Factors(encoding_dim)\n",
        "  model.to(device) ## 수정\n",
        "  print(model)\n",
        "\n",
        "\n",
        "  num_workers = 0\n",
        "  x_batch = no_of_stocks\n",
        "  y_batch = no_of_stocks\n",
        "  x_loader = torch.utils.data.DataLoader(x, batch_size=x_batch, num_workers=num_workers)\n",
        "  y_loader = torch.utils.data.DataLoader(y, batch_size=y_batch, num_workers=num_workers)\n",
        "\n",
        "\n",
        "  # specify loss function\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  # specify loss function\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  #df=pd.DataFrame()\n",
        "\n",
        "\n",
        "  for _ in range(100):\n",
        "    print('This is the data: {} and time: {}. The last one is the 1000th'.format(name, _))\n",
        "    df_1=pd.DataFrame()\n",
        "    number = 100\n",
        "\n",
        "\n",
        "    for epoch in range(1, number+1):\n",
        "    # monitor training loss\n",
        "      train_loss = 0.0\n",
        "\n",
        "      for x, y in zip(x_loader, y_loader):\n",
        "          x=x.to(device)\n",
        "          y=y.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          outputs = model(x.float(), y.float())\n",
        "\n",
        "        # calculate the loss\n",
        "          loss = criterion(outputs, x.float())\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "          loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "          optimizer.step()\n",
        "        # update running training loss\n",
        "          train_loss += loss.item()\n",
        "\n",
        "      train_loss = train_loss/len(x_loader)\n",
        "      print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch,\n",
        "        train_loss\n",
        "        ))\n",
        "\n",
        "    for x, y in zip(x_loader, y_loader):\n",
        "        x=x.to(device)\n",
        "        y=y.to(device)\n",
        "\n",
        "        result = model(x.float(), y.float())\n",
        "\n",
        "        unnormal_result = result * (data.종가.std() + 1) + data.종가.mean()\n",
        "\n",
        "        print(unnormal_result.detach().cpu().numpy()[0,:5]) # 수정\n",
        "\n",
        "        #unnormal_result = result * (data.finalp.std() + 1) + data.finalp.mean()\n",
        "        #print(unnormal_result.detach().cpu().numpy()[0,:5])\n",
        "##################################################\n",
        "    aa=pd.DataFrame(unnormal_result.cpu().detach().numpy()) # 수정\n",
        "###################################################\n",
        "    result_df=aa.T\n",
        "    result_df['real_price']=data['종가']\n",
        "    result_df['a']=abs(result_df[0]-result_df['real_price'])\n",
        "    #AAA=AAA.sort_values('a')\n",
        "    result_df['Name']=data['Name']\n",
        "    result_df\n",
        "####################################################\n",
        "#####################################################\n",
        "    print()\n",
        "    headresult_df.head(11132)\n",
        "    head.reset_index(drop=True, inplace=True)\n",
        "    print()\n",
        "####################################################\n",
        "    tail=result_df.tail(11132)\n",
        "    tail.reset_index(drop=True, inplace=True)\n",
        "#####################################################\n",
        "    all_result=pd.concat([head, tail], axis=1)\n",
        "    df_1=pd.concat([df_1,all_result])\n",
        "    # df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    whole_data=pd.concat([whole_data, df_1], axis=0)\n",
        "    # whole_data.drop_duplicates(inplace=True)\n",
        "\n",
        "whole_data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sUMxH2Bcd6lH",
        "outputId": "2c355095-ed99-451f-e436-a32a0b200f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96278.6     16792.688   90983.67    34638.117 1111992.1  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 52. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96297.75    16799.219   90998.41    34648.742 1111999.5  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 53. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015474\n",
            "Epoch: 51 \tTraining Loss: 0.015474\n",
            "Epoch: 52 \tTraining Loss: 0.015475\n",
            "Epoch: 53 \tTraining Loss: 0.015475\n",
            "Epoch: 54 \tTraining Loss: 0.015477\n",
            "Epoch: 55 \tTraining Loss: 0.015478\n",
            "Epoch: 56 \tTraining Loss: 0.015480\n",
            "Epoch: 57 \tTraining Loss: 0.015482\n",
            "Epoch: 58 \tTraining Loss: 0.015482\n",
            "Epoch: 59 \tTraining Loss: 0.015480\n",
            "Epoch: 60 \tTraining Loss: 0.015477\n",
            "Epoch: 61 \tTraining Loss: 0.015474\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015475\n",
            "Epoch: 64 \tTraining Loss: 0.015477\n",
            "Epoch: 65 \tTraining Loss: 0.015477\n",
            "Epoch: 66 \tTraining Loss: 0.015475\n",
            "Epoch: 67 \tTraining Loss: 0.015474\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015474\n",
            "Epoch: 70 \tTraining Loss: 0.015475\n",
            "Epoch: 71 \tTraining Loss: 0.015475\n",
            "Epoch: 72 \tTraining Loss: 0.015474\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015474\n",
            "Epoch: 76 \tTraining Loss: 0.015474\n",
            "Epoch: 77 \tTraining Loss: 0.015474\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015474\n",
            "Epoch: 82 \tTraining Loss: 0.015474\n",
            "Epoch: 83 \tTraining Loss: 0.015474\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96351.3     16817.688   91040.16    34678.695 1112020.8  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 54. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96300.016   16799.984   91000.016   34650.023 1112000.1  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 55. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015474\n",
            "Epoch: 94 \tTraining Loss: 0.015474\n",
            "Epoch: 95 \tTraining Loss: 0.015474\n",
            "Epoch: 96 \tTraining Loss: 0.015475\n",
            "Epoch: 97 \tTraining Loss: 0.015476\n",
            "Epoch: 98 \tTraining Loss: 0.015477\n",
            "Epoch: 99 \tTraining Loss: 0.015479\n",
            "Epoch: 100 \tTraining Loss: 0.015480\n",
            "[  97494.89    17214.922   91937.3     35322.266 1112483.   ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 56. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015481\n",
            "Epoch: 2 \tTraining Loss: 0.015480\n",
            "Epoch: 3 \tTraining Loss: 0.015477\n",
            "Epoch: 4 \tTraining Loss: 0.015475\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015475\n",
            "Epoch: 8 \tTraining Loss: 0.015476\n",
            "Epoch: 9 \tTraining Loss: 0.015476\n",
            "Epoch: 10 \tTraining Loss: 0.015475\n",
            "Epoch: 11 \tTraining Loss: 0.015474\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015474\n",
            "Epoch: 15 \tTraining Loss: 0.015475\n",
            "Epoch: 16 \tTraining Loss: 0.015474\n",
            "Epoch: 17 \tTraining Loss: 0.015474\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015474\n",
            "Epoch: 21 \tTraining Loss: 0.015474\n",
            "Epoch: 22 \tTraining Loss: 0.015474\n",
            "Epoch: 23 \tTraining Loss: 0.015474\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015474\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96305.78    16802.031   91004.54    34653.227 1112002.5  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 57. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015474\n",
            "[  96639.69    16919.047   91268.47    34842.625 1112139.4  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 58. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015474\n",
            "Epoch: 2 \tTraining Loss: 0.015474\n",
            "Epoch: 3 \tTraining Loss: 0.015475\n",
            "Epoch: 4 \tTraining Loss: 0.015475\n",
            "Epoch: 5 \tTraining Loss: 0.015476\n",
            "Epoch: 6 \tTraining Loss: 0.015478\n",
            "Epoch: 7 \tTraining Loss: 0.015479\n",
            "Epoch: 8 \tTraining Loss: 0.015481\n",
            "Epoch: 9 \tTraining Loss: 0.015481\n",
            "Epoch: 10 \tTraining Loss: 0.015479\n",
            "Epoch: 11 \tTraining Loss: 0.015477\n",
            "Epoch: 12 \tTraining Loss: 0.015474\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015474\n",
            "Epoch: 15 \tTraining Loss: 0.015475\n",
            "Epoch: 16 \tTraining Loss: 0.015476\n",
            "Epoch: 17 \tTraining Loss: 0.015476\n",
            "Epoch: 18 \tTraining Loss: 0.015475\n",
            "Epoch: 19 \tTraining Loss: 0.015474\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015474\n",
            "Epoch: 22 \tTraining Loss: 0.015474\n",
            "Epoch: 23 \tTraining Loss: 0.015475\n",
            "Epoch: 24 \tTraining Loss: 0.015474\n",
            "Epoch: 25 \tTraining Loss: 0.015474\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015474\n",
            "Epoch: 29 \tTraining Loss: 0.015474\n",
            "Epoch: 30 \tTraining Loss: 0.015474\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015474\n",
            "Epoch: 36 \tTraining Loss: 0.015474\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96290.59   16796.64   90992.45   34644.64 1111996.  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 59. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96275.625   16791.234   90980.375   34635.938 1111989.8  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 60. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015474\n",
            "Epoch: 13 \tTraining Loss: 0.015474\n",
            "Epoch: 14 \tTraining Loss: 0.015474\n",
            "Epoch: 15 \tTraining Loss: 0.015475\n",
            "Epoch: 16 \tTraining Loss: 0.015475\n",
            "Epoch: 17 \tTraining Loss: 0.015477\n",
            "Epoch: 18 \tTraining Loss: 0.015478\n",
            "Epoch: 19 \tTraining Loss: 0.015480\n",
            "Epoch: 20 \tTraining Loss: 0.015482\n",
            "Epoch: 21 \tTraining Loss: 0.015482\n",
            "Epoch: 22 \tTraining Loss: 0.015480\n",
            "Epoch: 23 \tTraining Loss: 0.015477\n",
            "Epoch: 24 \tTraining Loss: 0.015474\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015474\n",
            "Epoch: 27 \tTraining Loss: 0.015476\n",
            "Epoch: 28 \tTraining Loss: 0.015477\n",
            "Epoch: 29 \tTraining Loss: 0.015476\n",
            "Epoch: 30 \tTraining Loss: 0.015475\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015474\n",
            "Epoch: 34 \tTraining Loss: 0.015475\n",
            "Epoch: 35 \tTraining Loss: 0.015475\n",
            "Epoch: 36 \tTraining Loss: 0.015474\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015474\n",
            "Epoch: 40 \tTraining Loss: 0.015474\n",
            "Epoch: 41 \tTraining Loss: 0.015474\n",
            "Epoch: 42 \tTraining Loss: 0.015474\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015474\n",
            "Epoch: 47 \tTraining Loss: 0.015474\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96295.      16798.094   90995.84    34646.96  1111998.   ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 61. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96298.64    16799.547   90998.94    34649.195 1111999.2  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 62. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015474\n",
            "Epoch: 38 \tTraining Loss: 0.015474\n",
            "Epoch: 39 \tTraining Loss: 0.015474\n",
            "Epoch: 40 \tTraining Loss: 0.015475\n",
            "Epoch: 41 \tTraining Loss: 0.015476\n",
            "Epoch: 42 \tTraining Loss: 0.015477\n",
            "Epoch: 43 \tTraining Loss: 0.015479\n",
            "Epoch: 44 \tTraining Loss: 0.015481\n",
            "Epoch: 45 \tTraining Loss: 0.015482\n",
            "Epoch: 46 \tTraining Loss: 0.015481\n",
            "Epoch: 47 \tTraining Loss: 0.015479\n",
            "Epoch: 48 \tTraining Loss: 0.015476\n",
            "Epoch: 49 \tTraining Loss: 0.015474\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015474\n",
            "Epoch: 52 \tTraining Loss: 0.015476\n",
            "Epoch: 53 \tTraining Loss: 0.015477\n",
            "Epoch: 54 \tTraining Loss: 0.015476\n",
            "Epoch: 55 \tTraining Loss: 0.015474\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015474\n",
            "Epoch: 59 \tTraining Loss: 0.015475\n",
            "Epoch: 60 \tTraining Loss: 0.015475\n",
            "Epoch: 61 \tTraining Loss: 0.015474\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015474\n",
            "Epoch: 65 \tTraining Loss: 0.015474\n",
            "Epoch: 66 \tTraining Loss: 0.015474\n",
            "Epoch: 67 \tTraining Loss: 0.015474\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015474\n",
            "Epoch: 72 \tTraining Loss: 0.015474\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96331.36    16811.5     91025.484   34668.312 1112013.4  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 63. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96300.8    16800.25   91000.57   34650.33 1112000.2 ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 64. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015474\n",
            "Epoch: 57 \tTraining Loss: 0.015474\n",
            "Epoch: 58 \tTraining Loss: 0.015474\n",
            "Epoch: 59 \tTraining Loss: 0.015475\n",
            "Epoch: 60 \tTraining Loss: 0.015476\n",
            "Epoch: 61 \tTraining Loss: 0.015478\n",
            "Epoch: 62 \tTraining Loss: 0.015479\n",
            "Epoch: 63 \tTraining Loss: 0.015481\n",
            "Epoch: 64 \tTraining Loss: 0.015482\n",
            "Epoch: 65 \tTraining Loss: 0.015481\n",
            "Epoch: 66 \tTraining Loss: 0.015479\n",
            "Epoch: 67 \tTraining Loss: 0.015476\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015475\n",
            "Epoch: 71 \tTraining Loss: 0.015476\n",
            "Epoch: 72 \tTraining Loss: 0.015477\n",
            "Epoch: 73 \tTraining Loss: 0.015476\n",
            "Epoch: 74 \tTraining Loss: 0.015474\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015474\n",
            "Epoch: 77 \tTraining Loss: 0.015474\n",
            "Epoch: 78 \tTraining Loss: 0.015475\n",
            "Epoch: 79 \tTraining Loss: 0.015475\n",
            "Epoch: 80 \tTraining Loss: 0.015474\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015474\n",
            "Epoch: 84 \tTraining Loss: 0.015474\n",
            "Epoch: 85 \tTraining Loss: 0.015474\n",
            "Epoch: 86 \tTraining Loss: 0.015474\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015474\n",
            "Epoch: 91 \tTraining Loss: 0.015474\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96194.94    16761.297   90913.94    34588.4   1111955.6  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 65. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96299.6     16799.906   90999.766   34649.79  1111999.9  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 66. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015474\n",
            "Epoch: 77 \tTraining Loss: 0.015474\n",
            "Epoch: 78 \tTraining Loss: 0.015474\n",
            "Epoch: 79 \tTraining Loss: 0.015475\n",
            "Epoch: 80 \tTraining Loss: 0.015476\n",
            "Epoch: 81 \tTraining Loss: 0.015478\n",
            "Epoch: 82 \tTraining Loss: 0.015480\n",
            "Epoch: 83 \tTraining Loss: 0.015482\n",
            "Epoch: 84 \tTraining Loss: 0.015483\n",
            "Epoch: 85 \tTraining Loss: 0.015482\n",
            "Epoch: 86 \tTraining Loss: 0.015480\n",
            "Epoch: 87 \tTraining Loss: 0.015476\n",
            "Epoch: 88 \tTraining Loss: 0.015474\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015475\n",
            "Epoch: 91 \tTraining Loss: 0.015477\n",
            "Epoch: 92 \tTraining Loss: 0.015477\n",
            "Epoch: 93 \tTraining Loss: 0.015476\n",
            "Epoch: 94 \tTraining Loss: 0.015474\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015474\n",
            "Epoch: 97 \tTraining Loss: 0.015475\n",
            "Epoch: 98 \tTraining Loss: 0.015475\n",
            "Epoch: 99 \tTraining Loss: 0.015475\n",
            "Epoch: 100 \tTraining Loss: 0.015474\n",
            "[  96313.97    16805.172   91011.34    34658.203 1112007.5  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 67. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015474\n",
            "Epoch: 4 \tTraining Loss: 0.015474\n",
            "Epoch: 5 \tTraining Loss: 0.015474\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015474\n",
            "Epoch: 10 \tTraining Loss: 0.015474\n",
            "Epoch: 11 \tTraining Loss: 0.015474\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96296.09    16798.594   90996.66    34647.71  1111998.5  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 68. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015474\n",
            "Epoch: 100 \tTraining Loss: 0.015474\n",
            "[  95908.75    16654.516   90678.086   34419.17  1111837.4  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 69. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015474\n",
            "Epoch: 2 \tTraining Loss: 0.015475\n",
            "Epoch: 3 \tTraining Loss: 0.015475\n",
            "Epoch: 4 \tTraining Loss: 0.015476\n",
            "Epoch: 5 \tTraining Loss: 0.015477\n",
            "Epoch: 6 \tTraining Loss: 0.015479\n",
            "Epoch: 7 \tTraining Loss: 0.015480\n",
            "Epoch: 8 \tTraining Loss: 0.015481\n",
            "Epoch: 9 \tTraining Loss: 0.015480\n",
            "Epoch: 10 \tTraining Loss: 0.015478\n",
            "Epoch: 11 \tTraining Loss: 0.015476\n",
            "Epoch: 12 \tTraining Loss: 0.015474\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015474\n",
            "Epoch: 15 \tTraining Loss: 0.015475\n",
            "Epoch: 16 \tTraining Loss: 0.015476\n",
            "Epoch: 17 \tTraining Loss: 0.015476\n",
            "Epoch: 18 \tTraining Loss: 0.015475\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015474\n",
            "Epoch: 22 \tTraining Loss: 0.015474\n",
            "Epoch: 23 \tTraining Loss: 0.015475\n",
            "Epoch: 24 \tTraining Loss: 0.015474\n",
            "Epoch: 25 \tTraining Loss: 0.015474\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015474\n",
            "Epoch: 29 \tTraining Loss: 0.015474\n",
            "Epoch: 30 \tTraining Loss: 0.015474\n",
            "Epoch: 31 \tTraining Loss: 0.015474\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015474\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96296.92    16798.875   90997.305   34648.156 1111998.5  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 70. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96248.03    16780.312   90956.89    34618.844 1111979.1  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 71. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015474\n",
            "Epoch: 9 \tTraining Loss: 0.015474\n",
            "Epoch: 10 \tTraining Loss: 0.015475\n",
            "Epoch: 11 \tTraining Loss: 0.015476\n",
            "Epoch: 12 \tTraining Loss: 0.015478\n",
            "Epoch: 13 \tTraining Loss: 0.015480\n",
            "Epoch: 14 \tTraining Loss: 0.015483\n",
            "Epoch: 15 \tTraining Loss: 0.015486\n",
            "Epoch: 16 \tTraining Loss: 0.015487\n",
            "Epoch: 17 \tTraining Loss: 0.015484\n",
            "Epoch: 18 \tTraining Loss: 0.015478\n",
            "Epoch: 19 \tTraining Loss: 0.015474\n",
            "Epoch: 20 \tTraining Loss: 0.015474\n",
            "Epoch: 21 \tTraining Loss: 0.015476\n",
            "Epoch: 22 \tTraining Loss: 0.015479\n",
            "Epoch: 23 \tTraining Loss: 0.015479\n",
            "Epoch: 24 \tTraining Loss: 0.015476\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015474\n",
            "Epoch: 27 \tTraining Loss: 0.015476\n",
            "Epoch: 28 \tTraining Loss: 0.015476\n",
            "Epoch: 29 \tTraining Loss: 0.015475\n",
            "Epoch: 30 \tTraining Loss: 0.015474\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015474\n",
            "Epoch: 33 \tTraining Loss: 0.015475\n",
            "Epoch: 34 \tTraining Loss: 0.015475\n",
            "Epoch: 35 \tTraining Loss: 0.015474\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015474\n",
            "Epoch: 38 \tTraining Loss: 0.015474\n",
            "Epoch: 39 \tTraining Loss: 0.015474\n",
            "Epoch: 40 \tTraining Loss: 0.015474\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015474\n",
            "Epoch: 44 \tTraining Loss: 0.015474\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015474\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96307.4     16802.781   91006.17    34654.46  1112002.9  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 72. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96300.64    16800.188   91000.42    34650.35  1112000.1  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 73. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015474\n",
            "Epoch: 75 \tTraining Loss: 0.015474\n",
            "Epoch: 76 \tTraining Loss: 0.015474\n",
            "Epoch: 77 \tTraining Loss: 0.015475\n",
            "Epoch: 78 \tTraining Loss: 0.015475\n",
            "Epoch: 79 \tTraining Loss: 0.015476\n",
            "Epoch: 80 \tTraining Loss: 0.015477\n",
            "Epoch: 81 \tTraining Loss: 0.015479\n",
            "Epoch: 82 \tTraining Loss: 0.015480\n",
            "Epoch: 83 \tTraining Loss: 0.015481\n",
            "Epoch: 84 \tTraining Loss: 0.015481\n",
            "Epoch: 85 \tTraining Loss: 0.015479\n",
            "Epoch: 86 \tTraining Loss: 0.015476\n",
            "Epoch: 87 \tTraining Loss: 0.015474\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015474\n",
            "Epoch: 90 \tTraining Loss: 0.015475\n",
            "Epoch: 91 \tTraining Loss: 0.015476\n",
            "Epoch: 92 \tTraining Loss: 0.015476\n",
            "Epoch: 93 \tTraining Loss: 0.015475\n",
            "Epoch: 94 \tTraining Loss: 0.015474\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015474\n",
            "Epoch: 98 \tTraining Loss: 0.015475\n",
            "Epoch: 99 \tTraining Loss: 0.015474\n",
            "Epoch: 100 \tTraining Loss: 0.015474\n",
            "[  96476.28    16867.438   91147.36    34755.445 1112069.9  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 74. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015474\n",
            "Epoch: 5 \tTraining Loss: 0.015474\n",
            "Epoch: 6 \tTraining Loss: 0.015474\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015474\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96302.83    16801.062   91002.41    34651.78  1112001.1  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 75. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015474\n",
            "Epoch: 67 \tTraining Loss: 0.015474\n",
            "Epoch: 68 \tTraining Loss: 0.015474\n",
            "Epoch: 69 \tTraining Loss: 0.015475\n",
            "Epoch: 70 \tTraining Loss: 0.015476\n",
            "Epoch: 71 \tTraining Loss: 0.015478\n",
            "Epoch: 72 \tTraining Loss: 0.015480\n",
            "Epoch: 73 \tTraining Loss: 0.015482\n",
            "Epoch: 74 \tTraining Loss: 0.015483\n",
            "Epoch: 75 \tTraining Loss: 0.015483\n",
            "Epoch: 76 \tTraining Loss: 0.015480\n",
            "Epoch: 77 \tTraining Loss: 0.015477\n",
            "Epoch: 78 \tTraining Loss: 0.015474\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015475\n",
            "Epoch: 81 \tTraining Loss: 0.015476\n",
            "Epoch: 82 \tTraining Loss: 0.015477\n",
            "Epoch: 83 \tTraining Loss: 0.015476\n",
            "Epoch: 84 \tTraining Loss: 0.015474\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015474\n",
            "Epoch: 88 \tTraining Loss: 0.015475\n",
            "Epoch: 89 \tTraining Loss: 0.015475\n",
            "Epoch: 90 \tTraining Loss: 0.015474\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015474\n",
            "Epoch: 94 \tTraining Loss: 0.015474\n",
            "Epoch: 95 \tTraining Loss: 0.015474\n",
            "Epoch: 96 \tTraining Loss: 0.015474\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015474\n",
            "[  96002.83    16684.39    90750.45    34471.242 1111884.9  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 76. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015474\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96302.14    16800.781   91001.67    34651.344 1112000.6  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 77. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015474\n",
            "Epoch: 83 \tTraining Loss: 0.015474\n",
            "Epoch: 84 \tTraining Loss: 0.015474\n",
            "Epoch: 85 \tTraining Loss: 0.015475\n",
            "Epoch: 86 \tTraining Loss: 0.015475\n",
            "Epoch: 87 \tTraining Loss: 0.015477\n",
            "Epoch: 88 \tTraining Loss: 0.015478\n",
            "Epoch: 89 \tTraining Loss: 0.015480\n",
            "Epoch: 90 \tTraining Loss: 0.015482\n",
            "Epoch: 91 \tTraining Loss: 0.015484\n",
            "Epoch: 92 \tTraining Loss: 0.015483\n",
            "Epoch: 93 \tTraining Loss: 0.015480\n",
            "Epoch: 94 \tTraining Loss: 0.015476\n",
            "Epoch: 95 \tTraining Loss: 0.015474\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015475\n",
            "Epoch: 98 \tTraining Loss: 0.015477\n",
            "Epoch: 99 \tTraining Loss: 0.015477\n",
            "Epoch: 100 \tTraining Loss: 0.015476\n",
            "[  96695.234   16957.688   91333.45    34892.305 1112147.1  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 78. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015474\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015474\n",
            "Epoch: 4 \tTraining Loss: 0.015475\n",
            "Epoch: 5 \tTraining Loss: 0.015475\n",
            "Epoch: 6 \tTraining Loss: 0.015475\n",
            "Epoch: 7 \tTraining Loss: 0.015474\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015474\n",
            "Epoch: 11 \tTraining Loss: 0.015474\n",
            "Epoch: 12 \tTraining Loss: 0.015474\n",
            "Epoch: 13 \tTraining Loss: 0.015474\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015474\n",
            "Epoch: 17 \tTraining Loss: 0.015474\n",
            "Epoch: 18 \tTraining Loss: 0.015474\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96296.27    16798.484   90996.95    34647.65  1111998.9  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 79. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96207.42    16762.719   90921.875   34592.305 1111967.2  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 80. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015474\n",
            "Epoch: 7 \tTraining Loss: 0.015474\n",
            "Epoch: 8 \tTraining Loss: 0.015475\n",
            "Epoch: 9 \tTraining Loss: 0.015475\n",
            "Epoch: 10 \tTraining Loss: 0.015477\n",
            "Epoch: 11 \tTraining Loss: 0.015479\n",
            "Epoch: 12 \tTraining Loss: 0.015481\n",
            "Epoch: 13 \tTraining Loss: 0.015483\n",
            "Epoch: 14 \tTraining Loss: 0.015485\n",
            "Epoch: 15 \tTraining Loss: 0.015485\n",
            "Epoch: 16 \tTraining Loss: 0.015481\n",
            "Epoch: 17 \tTraining Loss: 0.015477\n",
            "Epoch: 18 \tTraining Loss: 0.015474\n",
            "Epoch: 19 \tTraining Loss: 0.015474\n",
            "Epoch: 20 \tTraining Loss: 0.015476\n",
            "Epoch: 21 \tTraining Loss: 0.015478\n",
            "Epoch: 22 \tTraining Loss: 0.015478\n",
            "Epoch: 23 \tTraining Loss: 0.015476\n",
            "Epoch: 24 \tTraining Loss: 0.015474\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015474\n",
            "Epoch: 27 \tTraining Loss: 0.015475\n",
            "Epoch: 28 \tTraining Loss: 0.015476\n",
            "Epoch: 29 \tTraining Loss: 0.015475\n",
            "Epoch: 30 \tTraining Loss: 0.015474\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015474\n",
            "Epoch: 33 \tTraining Loss: 0.015474\n",
            "Epoch: 34 \tTraining Loss: 0.015474\n",
            "Epoch: 35 \tTraining Loss: 0.015474\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015474\n",
            "Epoch: 39 \tTraining Loss: 0.015474\n",
            "Epoch: 40 \tTraining Loss: 0.015474\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015474\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96290.05    16796.016   90991.5     34643.984 1111996.4  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 81. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96303.05    16801.281   91002.78    34651.93  1112001.   ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 82. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015474\n",
            "Epoch: 33 \tTraining Loss: 0.015474\n",
            "Epoch: 34 \tTraining Loss: 0.015474\n",
            "Epoch: 35 \tTraining Loss: 0.015474\n",
            "Epoch: 36 \tTraining Loss: 0.015475\n",
            "Epoch: 37 \tTraining Loss: 0.015476\n",
            "Epoch: 38 \tTraining Loss: 0.015477\n",
            "Epoch: 39 \tTraining Loss: 0.015479\n",
            "Epoch: 40 \tTraining Loss: 0.015480\n",
            "Epoch: 41 \tTraining Loss: 0.015481\n",
            "Epoch: 42 \tTraining Loss: 0.015481\n",
            "Epoch: 43 \tTraining Loss: 0.015480\n",
            "Epoch: 44 \tTraining Loss: 0.015477\n",
            "Epoch: 45 \tTraining Loss: 0.015475\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015475\n",
            "Epoch: 49 \tTraining Loss: 0.015476\n",
            "Epoch: 50 \tTraining Loss: 0.015476\n",
            "Epoch: 51 \tTraining Loss: 0.015476\n",
            "Epoch: 52 \tTraining Loss: 0.015475\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015474\n",
            "Epoch: 56 \tTraining Loss: 0.015474\n",
            "Epoch: 57 \tTraining Loss: 0.015475\n",
            "Epoch: 58 \tTraining Loss: 0.015474\n",
            "Epoch: 59 \tTraining Loss: 0.015474\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015474\n",
            "Epoch: 64 \tTraining Loss: 0.015474\n",
            "Epoch: 65 \tTraining Loss: 0.015474\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96352.03    16820.797   91044.85    34681.17  1112018.6  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 83. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96310.79    16804.578   91009.36    34656.97  1112003.6  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 84. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015474\n",
            "Epoch: 19 \tTraining Loss: 0.015474\n",
            "Epoch: 20 \tTraining Loss: 0.015474\n",
            "Epoch: 21 \tTraining Loss: 0.015475\n",
            "Epoch: 22 \tTraining Loss: 0.015476\n",
            "Epoch: 23 \tTraining Loss: 0.015478\n",
            "Epoch: 24 \tTraining Loss: 0.015480\n",
            "Epoch: 25 \tTraining Loss: 0.015482\n",
            "Epoch: 26 \tTraining Loss: 0.015484\n",
            "Epoch: 27 \tTraining Loss: 0.015485\n",
            "Epoch: 28 \tTraining Loss: 0.015482\n",
            "Epoch: 29 \tTraining Loss: 0.015478\n",
            "Epoch: 30 \tTraining Loss: 0.015474\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015474\n",
            "Epoch: 33 \tTraining Loss: 0.015477\n",
            "Epoch: 34 \tTraining Loss: 0.015478\n",
            "Epoch: 35 \tTraining Loss: 0.015477\n",
            "Epoch: 36 \tTraining Loss: 0.015475\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015474\n",
            "Epoch: 40 \tTraining Loss: 0.015475\n",
            "Epoch: 41 \tTraining Loss: 0.015475\n",
            "Epoch: 42 \tTraining Loss: 0.015474\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015474\n",
            "Epoch: 46 \tTraining Loss: 0.015474\n",
            "Epoch: 47 \tTraining Loss: 0.015474\n",
            "Epoch: 48 \tTraining Loss: 0.015474\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015474\n",
            "Epoch: 53 \tTraining Loss: 0.015474\n",
            "Epoch: 54 \tTraining Loss: 0.015474\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96290.39    16796.562   90991.41    34645.04  1111995.6  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 85. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96293.28    16797.484   90994.016   34646.36  1111997.4  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 86. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015474\n",
            "Epoch: 29 \tTraining Loss: 0.015474\n",
            "Epoch: 30 \tTraining Loss: 0.015474\n",
            "Epoch: 31 \tTraining Loss: 0.015474\n",
            "Epoch: 32 \tTraining Loss: 0.015475\n",
            "Epoch: 33 \tTraining Loss: 0.015475\n",
            "Epoch: 34 \tTraining Loss: 0.015476\n",
            "Epoch: 35 \tTraining Loss: 0.015478\n",
            "Epoch: 36 \tTraining Loss: 0.015479\n",
            "Epoch: 37 \tTraining Loss: 0.015480\n",
            "Epoch: 38 \tTraining Loss: 0.015481\n",
            "Epoch: 39 \tTraining Loss: 0.015481\n",
            "Epoch: 40 \tTraining Loss: 0.015479\n",
            "Epoch: 41 \tTraining Loss: 0.015477\n",
            "Epoch: 42 \tTraining Loss: 0.015474\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015474\n",
            "Epoch: 46 \tTraining Loss: 0.015475\n",
            "Epoch: 47 \tTraining Loss: 0.015476\n",
            "Epoch: 48 \tTraining Loss: 0.015476\n",
            "Epoch: 49 \tTraining Loss: 0.015475\n",
            "Epoch: 50 \tTraining Loss: 0.015474\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015474\n",
            "Epoch: 54 \tTraining Loss: 0.015474\n",
            "Epoch: 55 \tTraining Loss: 0.015474\n",
            "Epoch: 56 \tTraining Loss: 0.015474\n",
            "Epoch: 57 \tTraining Loss: 0.015474\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015474\n",
            "Epoch: 62 \tTraining Loss: 0.015474\n",
            "Epoch: 63 \tTraining Loss: 0.015474\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96272.336   16789.953   90974.875   34635.61  1111988.4  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 87. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96163.73    16743.578   90881.14    34565.992 1111956.5  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 88. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015474\n",
            "Epoch: 5 \tTraining Loss: 0.015474\n",
            "Epoch: 6 \tTraining Loss: 0.015474\n",
            "Epoch: 7 \tTraining Loss: 0.015475\n",
            "Epoch: 8 \tTraining Loss: 0.015477\n",
            "Epoch: 9 \tTraining Loss: 0.015478\n",
            "Epoch: 10 \tTraining Loss: 0.015481\n",
            "Epoch: 11 \tTraining Loss: 0.015484\n",
            "Epoch: 12 \tTraining Loss: 0.015487\n",
            "Epoch: 13 \tTraining Loss: 0.015487\n",
            "Epoch: 14 \tTraining Loss: 0.015485\n",
            "Epoch: 15 \tTraining Loss: 0.015479\n",
            "Epoch: 16 \tTraining Loss: 0.015475\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015475\n",
            "Epoch: 19 \tTraining Loss: 0.015478\n",
            "Epoch: 20 \tTraining Loss: 0.015479\n",
            "Epoch: 21 \tTraining Loss: 0.015478\n",
            "Epoch: 22 \tTraining Loss: 0.015475\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015474\n",
            "Epoch: 25 \tTraining Loss: 0.015476\n",
            "Epoch: 26 \tTraining Loss: 0.015476\n",
            "Epoch: 27 \tTraining Loss: 0.015475\n",
            "Epoch: 28 \tTraining Loss: 0.015474\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015474\n",
            "Epoch: 31 \tTraining Loss: 0.015475\n",
            "Epoch: 32 \tTraining Loss: 0.015475\n",
            "Epoch: 33 \tTraining Loss: 0.015474\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015474\n",
            "Epoch: 37 \tTraining Loss: 0.015474\n",
            "Epoch: 38 \tTraining Loss: 0.015474\n",
            "Epoch: 39 \tTraining Loss: 0.015474\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015474\n",
            "Epoch: 43 \tTraining Loss: 0.015474\n",
            "Epoch: 44 \tTraining Loss: 0.015474\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96289.625   16795.531   90991.08    34643.242 1111997.1  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 89. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96301.86    16800.828   91001.52    34651.227 1112000.5  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 90. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015474\n",
            "Epoch: 44 \tTraining Loss: 0.015474\n",
            "Epoch: 45 \tTraining Loss: 0.015474\n",
            "Epoch: 46 \tTraining Loss: 0.015475\n",
            "Epoch: 47 \tTraining Loss: 0.015476\n",
            "Epoch: 48 \tTraining Loss: 0.015477\n",
            "Epoch: 49 \tTraining Loss: 0.015479\n",
            "Epoch: 50 \tTraining Loss: 0.015482\n",
            "Epoch: 51 \tTraining Loss: 0.015485\n",
            "Epoch: 52 \tTraining Loss: 0.015486\n",
            "Epoch: 53 \tTraining Loss: 0.015485\n",
            "Epoch: 54 \tTraining Loss: 0.015482\n",
            "Epoch: 55 \tTraining Loss: 0.015477\n",
            "Epoch: 56 \tTraining Loss: 0.015474\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015475\n",
            "Epoch: 59 \tTraining Loss: 0.015478\n",
            "Epoch: 60 \tTraining Loss: 0.015478\n",
            "Epoch: 61 \tTraining Loss: 0.015477\n",
            "Epoch: 62 \tTraining Loss: 0.015474\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015474\n",
            "Epoch: 65 \tTraining Loss: 0.015475\n",
            "Epoch: 66 \tTraining Loss: 0.015476\n",
            "Epoch: 67 \tTraining Loss: 0.015475\n",
            "Epoch: 68 \tTraining Loss: 0.015474\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015474\n",
            "Epoch: 72 \tTraining Loss: 0.015475\n",
            "Epoch: 73 \tTraining Loss: 0.015474\n",
            "Epoch: 74 \tTraining Loss: 0.015474\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015474\n",
            "Epoch: 78 \tTraining Loss: 0.015474\n",
            "Epoch: 79 \tTraining Loss: 0.015474\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015474\n",
            "Epoch: 85 \tTraining Loss: 0.015474\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96337.45    16818.516   91031.47    34678.258 1112005.   ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 91. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96299.56    16799.906   90999.62    34649.92  1111999.6  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 92. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015474\n",
            "Epoch: 57 \tTraining Loss: 0.015474\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015474\n",
            "Epoch: 63 \tTraining Loss: 0.015474\n",
            "Epoch: 64 \tTraining Loss: 0.015474\n",
            "Epoch: 65 \tTraining Loss: 0.015475\n",
            "Epoch: 66 \tTraining Loss: 0.015476\n",
            "Epoch: 67 \tTraining Loss: 0.015477\n",
            "Epoch: 68 \tTraining Loss: 0.015478\n",
            "Epoch: 69 \tTraining Loss: 0.015480\n",
            "Epoch: 70 \tTraining Loss: 0.015482\n",
            "Epoch: 71 \tTraining Loss: 0.015484\n",
            "Epoch: 72 \tTraining Loss: 0.015484\n",
            "Epoch: 73 \tTraining Loss: 0.015483\n",
            "Epoch: 74 \tTraining Loss: 0.015479\n",
            "Epoch: 75 \tTraining Loss: 0.015475\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015474\n",
            "Epoch: 78 \tTraining Loss: 0.015476\n",
            "Epoch: 79 \tTraining Loss: 0.015477\n",
            "Epoch: 80 \tTraining Loss: 0.015478\n",
            "Epoch: 81 \tTraining Loss: 0.015476\n",
            "Epoch: 82 \tTraining Loss: 0.015474\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015474\n",
            "Epoch: 86 \tTraining Loss: 0.015475\n",
            "Epoch: 87 \tTraining Loss: 0.015475\n",
            "Epoch: 88 \tTraining Loss: 0.015474\n",
            "Epoch: 89 \tTraining Loss: 0.015474\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015474\n",
            "Epoch: 93 \tTraining Loss: 0.015474\n",
            "Epoch: 94 \tTraining Loss: 0.015474\n",
            "Epoch: 95 \tTraining Loss: 0.015474\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015474\n",
            "Epoch: 100 \tTraining Loss: 0.015474\n",
            "[  96056.45    16689.938   90783.94    34487.93  1111945.4  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 93. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015474\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96298.44    16799.219   90998.44    34648.883 1111999.8  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 94. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015474\n",
            "Epoch: 57 \tTraining Loss: 0.015474\n",
            "Epoch: 58 \tTraining Loss: 0.015474\n",
            "Epoch: 59 \tTraining Loss: 0.015475\n",
            "Epoch: 60 \tTraining Loss: 0.015476\n",
            "Epoch: 61 \tTraining Loss: 0.015477\n",
            "Epoch: 62 \tTraining Loss: 0.015479\n",
            "Epoch: 63 \tTraining Loss: 0.015482\n",
            "Epoch: 64 \tTraining Loss: 0.015484\n",
            "Epoch: 65 \tTraining Loss: 0.015486\n",
            "Epoch: 66 \tTraining Loss: 0.015485\n",
            "Epoch: 67 \tTraining Loss: 0.015482\n",
            "Epoch: 68 \tTraining Loss: 0.015478\n",
            "Epoch: 69 \tTraining Loss: 0.015474\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015474\n",
            "Epoch: 72 \tTraining Loss: 0.015477\n",
            "Epoch: 73 \tTraining Loss: 0.015478\n",
            "Epoch: 74 \tTraining Loss: 0.015478\n",
            "Epoch: 75 \tTraining Loss: 0.015476\n",
            "Epoch: 76 \tTraining Loss: 0.015474\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015474\n",
            "Epoch: 79 \tTraining Loss: 0.015475\n",
            "Epoch: 80 \tTraining Loss: 0.015476\n",
            "Epoch: 81 \tTraining Loss: 0.015475\n",
            "Epoch: 82 \tTraining Loss: 0.015474\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015474\n",
            "Epoch: 86 \tTraining Loss: 0.015474\n",
            "Epoch: 87 \tTraining Loss: 0.015474\n",
            "Epoch: 88 \tTraining Loss: 0.015474\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015474\n",
            "Epoch: 93 \tTraining Loss: 0.015474\n",
            "Epoch: 94 \tTraining Loss: 0.015474\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96471.82    16869.125   91162.7     34746.1   1112050.2  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 95. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96298.84    16799.219   90999.016   34648.89  1112000.   ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 96. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015474\n",
            "Epoch: 65 \tTraining Loss: 0.015474\n",
            "Epoch: 66 \tTraining Loss: 0.015475\n",
            "Epoch: 67 \tTraining Loss: 0.015475\n",
            "Epoch: 68 \tTraining Loss: 0.015477\n",
            "Epoch: 69 \tTraining Loss: 0.015479\n",
            "Epoch: 70 \tTraining Loss: 0.015482\n",
            "Epoch: 71 \tTraining Loss: 0.015487\n",
            "Epoch: 72 \tTraining Loss: 0.015491\n",
            "Epoch: 73 \tTraining Loss: 0.015493\n",
            "Epoch: 74 \tTraining Loss: 0.015490\n",
            "Epoch: 75 \tTraining Loss: 0.015483\n",
            "Epoch: 76 \tTraining Loss: 0.015476\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015476\n",
            "Epoch: 79 \tTraining Loss: 0.015480\n",
            "Epoch: 80 \tTraining Loss: 0.015481\n",
            "Epoch: 81 \tTraining Loss: 0.015479\n",
            "Epoch: 82 \tTraining Loss: 0.015475\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015474\n",
            "Epoch: 85 \tTraining Loss: 0.015477\n",
            "Epoch: 86 \tTraining Loss: 0.015477\n",
            "Epoch: 87 \tTraining Loss: 0.015476\n",
            "Epoch: 88 \tTraining Loss: 0.015474\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015474\n",
            "Epoch: 91 \tTraining Loss: 0.015475\n",
            "Epoch: 92 \tTraining Loss: 0.015475\n",
            "Epoch: 93 \tTraining Loss: 0.015474\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015474\n",
            "Epoch: 96 \tTraining Loss: 0.015474\n",
            "Epoch: 97 \tTraining Loss: 0.015474\n",
            "Epoch: 98 \tTraining Loss: 0.015474\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96066.77    16683.938   90795.984   34478.67  1111969.9  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 97. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015474\n",
            "Epoch: 2 \tTraining Loss: 0.015474\n",
            "Epoch: 3 \tTraining Loss: 0.015474\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015474\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96300.49    16800.328   90999.99    34650.562 1111999.9  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 98. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015473\n",
            "Epoch: 16 \tTraining Loss: 0.015473\n",
            "Epoch: 17 \tTraining Loss: 0.015473\n",
            "Epoch: 18 \tTraining Loss: 0.015473\n",
            "Epoch: 19 \tTraining Loss: 0.015473\n",
            "Epoch: 20 \tTraining Loss: 0.015473\n",
            "Epoch: 21 \tTraining Loss: 0.015473\n",
            "Epoch: 22 \tTraining Loss: 0.015473\n",
            "Epoch: 23 \tTraining Loss: 0.015473\n",
            "Epoch: 24 \tTraining Loss: 0.015473\n",
            "Epoch: 25 \tTraining Loss: 0.015473\n",
            "Epoch: 26 \tTraining Loss: 0.015473\n",
            "Epoch: 27 \tTraining Loss: 0.015473\n",
            "Epoch: 28 \tTraining Loss: 0.015473\n",
            "Epoch: 29 \tTraining Loss: 0.015473\n",
            "Epoch: 30 \tTraining Loss: 0.015473\n",
            "Epoch: 31 \tTraining Loss: 0.015473\n",
            "Epoch: 32 \tTraining Loss: 0.015473\n",
            "Epoch: 33 \tTraining Loss: 0.015473\n",
            "Epoch: 34 \tTraining Loss: 0.015473\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015473\n",
            "Epoch: 37 \tTraining Loss: 0.015473\n",
            "Epoch: 38 \tTraining Loss: 0.015473\n",
            "Epoch: 39 \tTraining Loss: 0.015473\n",
            "Epoch: 40 \tTraining Loss: 0.015473\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015473\n",
            "Epoch: 43 \tTraining Loss: 0.015473\n",
            "Epoch: 44 \tTraining Loss: 0.015473\n",
            "Epoch: 45 \tTraining Loss: 0.015473\n",
            "Epoch: 46 \tTraining Loss: 0.015473\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015473\n",
            "Epoch: 50 \tTraining Loss: 0.015473\n",
            "Epoch: 51 \tTraining Loss: 0.015473\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015473\n",
            "Epoch: 56 \tTraining Loss: 0.015473\n",
            "Epoch: 57 \tTraining Loss: 0.015473\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96314.055   16805.406   91013.734   34657.375 1112003.6  ]\n",
            "\n",
            "\n",
            "This is the data: aa and time: 99. The last one is the 1000th\n",
            "Epoch: 1 \tTraining Loss: 0.015473\n",
            "Epoch: 2 \tTraining Loss: 0.015473\n",
            "Epoch: 3 \tTraining Loss: 0.015473\n",
            "Epoch: 4 \tTraining Loss: 0.015473\n",
            "Epoch: 5 \tTraining Loss: 0.015473\n",
            "Epoch: 6 \tTraining Loss: 0.015473\n",
            "Epoch: 7 \tTraining Loss: 0.015473\n",
            "Epoch: 8 \tTraining Loss: 0.015473\n",
            "Epoch: 9 \tTraining Loss: 0.015473\n",
            "Epoch: 10 \tTraining Loss: 0.015473\n",
            "Epoch: 11 \tTraining Loss: 0.015473\n",
            "Epoch: 12 \tTraining Loss: 0.015473\n",
            "Epoch: 13 \tTraining Loss: 0.015473\n",
            "Epoch: 14 \tTraining Loss: 0.015473\n",
            "Epoch: 15 \tTraining Loss: 0.015474\n",
            "Epoch: 16 \tTraining Loss: 0.015474\n",
            "Epoch: 17 \tTraining Loss: 0.015474\n",
            "Epoch: 18 \tTraining Loss: 0.015475\n",
            "Epoch: 19 \tTraining Loss: 0.015477\n",
            "Epoch: 20 \tTraining Loss: 0.015479\n",
            "Epoch: 21 \tTraining Loss: 0.015481\n",
            "Epoch: 22 \tTraining Loss: 0.015485\n",
            "Epoch: 23 \tTraining Loss: 0.015488\n",
            "Epoch: 24 \tTraining Loss: 0.015491\n",
            "Epoch: 25 \tTraining Loss: 0.015490\n",
            "Epoch: 26 \tTraining Loss: 0.015486\n",
            "Epoch: 27 \tTraining Loss: 0.015479\n",
            "Epoch: 28 \tTraining Loss: 0.015474\n",
            "Epoch: 29 \tTraining Loss: 0.015474\n",
            "Epoch: 30 \tTraining Loss: 0.015476\n",
            "Epoch: 31 \tTraining Loss: 0.015480\n",
            "Epoch: 32 \tTraining Loss: 0.015480\n",
            "Epoch: 33 \tTraining Loss: 0.015478\n",
            "Epoch: 34 \tTraining Loss: 0.015475\n",
            "Epoch: 35 \tTraining Loss: 0.015473\n",
            "Epoch: 36 \tTraining Loss: 0.015474\n",
            "Epoch: 37 \tTraining Loss: 0.015476\n",
            "Epoch: 38 \tTraining Loss: 0.015477\n",
            "Epoch: 39 \tTraining Loss: 0.015476\n",
            "Epoch: 40 \tTraining Loss: 0.015474\n",
            "Epoch: 41 \tTraining Loss: 0.015473\n",
            "Epoch: 42 \tTraining Loss: 0.015474\n",
            "Epoch: 43 \tTraining Loss: 0.015475\n",
            "Epoch: 44 \tTraining Loss: 0.015475\n",
            "Epoch: 45 \tTraining Loss: 0.015475\n",
            "Epoch: 46 \tTraining Loss: 0.015474\n",
            "Epoch: 47 \tTraining Loss: 0.015473\n",
            "Epoch: 48 \tTraining Loss: 0.015473\n",
            "Epoch: 49 \tTraining Loss: 0.015474\n",
            "Epoch: 50 \tTraining Loss: 0.015474\n",
            "Epoch: 51 \tTraining Loss: 0.015474\n",
            "Epoch: 52 \tTraining Loss: 0.015473\n",
            "Epoch: 53 \tTraining Loss: 0.015473\n",
            "Epoch: 54 \tTraining Loss: 0.015473\n",
            "Epoch: 55 \tTraining Loss: 0.015474\n",
            "Epoch: 56 \tTraining Loss: 0.015474\n",
            "Epoch: 57 \tTraining Loss: 0.015474\n",
            "Epoch: 58 \tTraining Loss: 0.015473\n",
            "Epoch: 59 \tTraining Loss: 0.015473\n",
            "Epoch: 60 \tTraining Loss: 0.015473\n",
            "Epoch: 61 \tTraining Loss: 0.015473\n",
            "Epoch: 62 \tTraining Loss: 0.015473\n",
            "Epoch: 63 \tTraining Loss: 0.015473\n",
            "Epoch: 64 \tTraining Loss: 0.015473\n",
            "Epoch: 65 \tTraining Loss: 0.015473\n",
            "Epoch: 66 \tTraining Loss: 0.015473\n",
            "Epoch: 67 \tTraining Loss: 0.015473\n",
            "Epoch: 68 \tTraining Loss: 0.015473\n",
            "Epoch: 69 \tTraining Loss: 0.015473\n",
            "Epoch: 70 \tTraining Loss: 0.015473\n",
            "Epoch: 71 \tTraining Loss: 0.015473\n",
            "Epoch: 72 \tTraining Loss: 0.015473\n",
            "Epoch: 73 \tTraining Loss: 0.015473\n",
            "Epoch: 74 \tTraining Loss: 0.015473\n",
            "Epoch: 75 \tTraining Loss: 0.015473\n",
            "Epoch: 76 \tTraining Loss: 0.015473\n",
            "Epoch: 77 \tTraining Loss: 0.015473\n",
            "Epoch: 78 \tTraining Loss: 0.015473\n",
            "Epoch: 79 \tTraining Loss: 0.015473\n",
            "Epoch: 80 \tTraining Loss: 0.015473\n",
            "Epoch: 81 \tTraining Loss: 0.015473\n",
            "Epoch: 82 \tTraining Loss: 0.015473\n",
            "Epoch: 83 \tTraining Loss: 0.015473\n",
            "Epoch: 84 \tTraining Loss: 0.015473\n",
            "Epoch: 85 \tTraining Loss: 0.015473\n",
            "Epoch: 86 \tTraining Loss: 0.015473\n",
            "Epoch: 87 \tTraining Loss: 0.015473\n",
            "Epoch: 88 \tTraining Loss: 0.015473\n",
            "Epoch: 89 \tTraining Loss: 0.015473\n",
            "Epoch: 90 \tTraining Loss: 0.015473\n",
            "Epoch: 91 \tTraining Loss: 0.015473\n",
            "Epoch: 92 \tTraining Loss: 0.015473\n",
            "Epoch: 93 \tTraining Loss: 0.015473\n",
            "Epoch: 94 \tTraining Loss: 0.015473\n",
            "Epoch: 95 \tTraining Loss: 0.015473\n",
            "Epoch: 96 \tTraining Loss: 0.015473\n",
            "Epoch: 97 \tTraining Loss: 0.015473\n",
            "Epoch: 98 \tTraining Loss: 0.015473\n",
            "Epoch: 99 \tTraining Loss: 0.015473\n",
            "Epoch: 100 \tTraining Loss: 0.015473\n",
            "[  96327.      16811.781   91026.04    34666.4   1112006.   ]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   0  real_price              a            Name  \\\n",
              "0      112394.460938     96300.0   16094.460938        (주)케이티앤지   \n",
              "1      154718.968750     16800.0  137918.968750       (주)LG유플러스   \n",
              "2       98744.046875     91000.0    7744.046875        (주)현대백화점   \n",
              "3      151849.218750     34650.0  117199.218750           (주)한섬   \n",
              "4      208000.015625   1112000.0  903999.984375       (주)LG생활건강   \n",
              "...              ...         ...            ...             ...   \n",
              "11127   70891.117188     70900.0       8.882812          (주)이마트   \n",
              "11128   37172.367188     37150.0      22.367188  한국타이어앤테크놀로지(주)   \n",
              "11129  116215.335938    116200.0      15.335938          (주)오리온   \n",
              "11130  109008.218750    109000.0       8.218750        롯데웰푸드(주)   \n",
              "11131   92616.812500     92600.0      16.812500          (주)종근당   \n",
              "\n",
              "                   0  real_price              a            Name  \n",
              "0      112394.460938     96300.0   16094.460938        (주)케이티앤지  \n",
              "1      154718.968750     16800.0  137918.968750       (주)LG유플러스  \n",
              "2       98744.046875     91000.0    7744.046875        (주)현대백화점  \n",
              "3      151849.218750     34650.0  117199.218750           (주)한섬  \n",
              "4      208000.015625   1112000.0  903999.984375       (주)LG생활건강  \n",
              "...              ...         ...            ...             ...  \n",
              "11127   70891.117188     70900.0       8.882812          (주)이마트  \n",
              "11128   37172.367188     37150.0      22.367188  한국타이어앤테크놀로지(주)  \n",
              "11129  116215.335938    116200.0      15.335938          (주)오리온  \n",
              "11130  109008.218750    109000.0       8.218750        롯데웰푸드(주)  \n",
              "11131   92616.812500     92600.0      16.812500          (주)종근당  \n",
              "\n",
              "[1113200 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f89f7887-cd52-4a5c-9490-31b7beb086cf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>real_price</th>\n",
              "      <th>a</th>\n",
              "      <th>Name</th>\n",
              "      <th>0</th>\n",
              "      <th>real_price</th>\n",
              "      <th>a</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>112394.460938</td>\n",
              "      <td>96300.0</td>\n",
              "      <td>16094.460938</td>\n",
              "      <td>(주)케이티앤지</td>\n",
              "      <td>112394.460938</td>\n",
              "      <td>96300.0</td>\n",
              "      <td>16094.460938</td>\n",
              "      <td>(주)케이티앤지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>154718.968750</td>\n",
              "      <td>16800.0</td>\n",
              "      <td>137918.968750</td>\n",
              "      <td>(주)LG유플러스</td>\n",
              "      <td>154718.968750</td>\n",
              "      <td>16800.0</td>\n",
              "      <td>137918.968750</td>\n",
              "      <td>(주)LG유플러스</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>98744.046875</td>\n",
              "      <td>91000.0</td>\n",
              "      <td>7744.046875</td>\n",
              "      <td>(주)현대백화점</td>\n",
              "      <td>98744.046875</td>\n",
              "      <td>91000.0</td>\n",
              "      <td>7744.046875</td>\n",
              "      <td>(주)현대백화점</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151849.218750</td>\n",
              "      <td>34650.0</td>\n",
              "      <td>117199.218750</td>\n",
              "      <td>(주)한섬</td>\n",
              "      <td>151849.218750</td>\n",
              "      <td>34650.0</td>\n",
              "      <td>117199.218750</td>\n",
              "      <td>(주)한섬</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>208000.015625</td>\n",
              "      <td>1112000.0</td>\n",
              "      <td>903999.984375</td>\n",
              "      <td>(주)LG생활건강</td>\n",
              "      <td>208000.015625</td>\n",
              "      <td>1112000.0</td>\n",
              "      <td>903999.984375</td>\n",
              "      <td>(주)LG생활건강</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11127</th>\n",
              "      <td>70891.117188</td>\n",
              "      <td>70900.0</td>\n",
              "      <td>8.882812</td>\n",
              "      <td>(주)이마트</td>\n",
              "      <td>70891.117188</td>\n",
              "      <td>70900.0</td>\n",
              "      <td>8.882812</td>\n",
              "      <td>(주)이마트</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11128</th>\n",
              "      <td>37172.367188</td>\n",
              "      <td>37150.0</td>\n",
              "      <td>22.367188</td>\n",
              "      <td>한국타이어앤테크놀로지(주)</td>\n",
              "      <td>37172.367188</td>\n",
              "      <td>37150.0</td>\n",
              "      <td>22.367188</td>\n",
              "      <td>한국타이어앤테크놀로지(주)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11129</th>\n",
              "      <td>116215.335938</td>\n",
              "      <td>116200.0</td>\n",
              "      <td>15.335938</td>\n",
              "      <td>(주)오리온</td>\n",
              "      <td>116215.335938</td>\n",
              "      <td>116200.0</td>\n",
              "      <td>15.335938</td>\n",
              "      <td>(주)오리온</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11130</th>\n",
              "      <td>109008.218750</td>\n",
              "      <td>109000.0</td>\n",
              "      <td>8.218750</td>\n",
              "      <td>롯데웰푸드(주)</td>\n",
              "      <td>109008.218750</td>\n",
              "      <td>109000.0</td>\n",
              "      <td>8.218750</td>\n",
              "      <td>롯데웰푸드(주)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11131</th>\n",
              "      <td>92616.812500</td>\n",
              "      <td>92600.0</td>\n",
              "      <td>16.812500</td>\n",
              "      <td>(주)종근당</td>\n",
              "      <td>92616.812500</td>\n",
              "      <td>92600.0</td>\n",
              "      <td>16.812500</td>\n",
              "      <td>(주)종근당</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1113200 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f89f7887-cd52-4a5c-9490-31b7beb086cf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f89f7887-cd52-4a5c-9490-31b7beb086cf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f89f7887-cd52-4a5c-9490-31b7beb086cf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5b299d98-c3fe-447d-87ae-59421a14d2d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b299d98-c3fe-447d-87ae-59421a14d2d1')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5b299d98-c3fe-447d-87ae-59421a14d2d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#whole_data.to_csv('whole_data1.csv', encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "WjQzkyV8ha6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whole_data.columns=['top_predict_price', 'top_real_price','top_difference' ,'top_five_name', 'bottom_predict_price', 'bottom_real_price','bottom_difference','bottom_five_name']"
      ],
      "metadata": {
        "id": "012OU0JMUuG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whole_data"
      ],
      "metadata": {
        "id": "WDPD44jnre8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "c19f5eb8-fbd8-407a-9c51-cfe8ef3c855d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       top_predict_price  top_real_price  top_difference   top_five_name  \\\n",
              "0          112394.460938         96300.0    16094.460938        (주)케이티앤지   \n",
              "1          154718.968750         16800.0   137918.968750       (주)LG유플러스   \n",
              "2           98744.046875         91000.0     7744.046875        (주)현대백화점   \n",
              "3          151849.218750         34650.0   117199.218750           (주)한섬   \n",
              "4          208000.015625       1112000.0   903999.984375       (주)LG생활건강   \n",
              "...                  ...             ...             ...             ...   \n",
              "11127       70891.117188         70900.0        8.882812          (주)이마트   \n",
              "11128       37172.367188         37150.0       22.367188  한국타이어앤테크놀로지(주)   \n",
              "11129      116215.335938        116200.0       15.335938          (주)오리온   \n",
              "11130      109008.218750        109000.0        8.218750        롯데웰푸드(주)   \n",
              "11131       92616.812500         92600.0       16.812500          (주)종근당   \n",
              "\n",
              "       bottom_predict_price  bottom_real_price  bottom_difference  \\\n",
              "0             112394.460938            96300.0       16094.460938   \n",
              "1             154718.968750            16800.0      137918.968750   \n",
              "2              98744.046875            91000.0        7744.046875   \n",
              "3             151849.218750            34650.0      117199.218750   \n",
              "4             208000.015625          1112000.0      903999.984375   \n",
              "...                     ...                ...                ...   \n",
              "11127          70891.117188            70900.0           8.882812   \n",
              "11128          37172.367188            37150.0          22.367188   \n",
              "11129         116215.335938           116200.0          15.335938   \n",
              "11130         109008.218750           109000.0           8.218750   \n",
              "11131          92616.812500            92600.0          16.812500   \n",
              "\n",
              "      bottom_five_name  \n",
              "0             (주)케이티앤지  \n",
              "1            (주)LG유플러스  \n",
              "2             (주)현대백화점  \n",
              "3                (주)한섬  \n",
              "4            (주)LG생활건강  \n",
              "...                ...  \n",
              "11127           (주)이마트  \n",
              "11128   한국타이어앤테크놀로지(주)  \n",
              "11129           (주)오리온  \n",
              "11130         롯데웰푸드(주)  \n",
              "11131           (주)종근당  \n",
              "\n",
              "[1113200 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d4f2fda-ab8f-4ec5-b4ad-7e4e498de7c3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>top_predict_price</th>\n",
              "      <th>top_real_price</th>\n",
              "      <th>top_difference</th>\n",
              "      <th>top_five_name</th>\n",
              "      <th>bottom_predict_price</th>\n",
              "      <th>bottom_real_price</th>\n",
              "      <th>bottom_difference</th>\n",
              "      <th>bottom_five_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>112394.460938</td>\n",
              "      <td>96300.0</td>\n",
              "      <td>16094.460938</td>\n",
              "      <td>(주)케이티앤지</td>\n",
              "      <td>112394.460938</td>\n",
              "      <td>96300.0</td>\n",
              "      <td>16094.460938</td>\n",
              "      <td>(주)케이티앤지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>154718.968750</td>\n",
              "      <td>16800.0</td>\n",
              "      <td>137918.968750</td>\n",
              "      <td>(주)LG유플러스</td>\n",
              "      <td>154718.968750</td>\n",
              "      <td>16800.0</td>\n",
              "      <td>137918.968750</td>\n",
              "      <td>(주)LG유플러스</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>98744.046875</td>\n",
              "      <td>91000.0</td>\n",
              "      <td>7744.046875</td>\n",
              "      <td>(주)현대백화점</td>\n",
              "      <td>98744.046875</td>\n",
              "      <td>91000.0</td>\n",
              "      <td>7744.046875</td>\n",
              "      <td>(주)현대백화점</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151849.218750</td>\n",
              "      <td>34650.0</td>\n",
              "      <td>117199.218750</td>\n",
              "      <td>(주)한섬</td>\n",
              "      <td>151849.218750</td>\n",
              "      <td>34650.0</td>\n",
              "      <td>117199.218750</td>\n",
              "      <td>(주)한섬</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>208000.015625</td>\n",
              "      <td>1112000.0</td>\n",
              "      <td>903999.984375</td>\n",
              "      <td>(주)LG생활건강</td>\n",
              "      <td>208000.015625</td>\n",
              "      <td>1112000.0</td>\n",
              "      <td>903999.984375</td>\n",
              "      <td>(주)LG생활건강</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11127</th>\n",
              "      <td>70891.117188</td>\n",
              "      <td>70900.0</td>\n",
              "      <td>8.882812</td>\n",
              "      <td>(주)이마트</td>\n",
              "      <td>70891.117188</td>\n",
              "      <td>70900.0</td>\n",
              "      <td>8.882812</td>\n",
              "      <td>(주)이마트</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11128</th>\n",
              "      <td>37172.367188</td>\n",
              "      <td>37150.0</td>\n",
              "      <td>22.367188</td>\n",
              "      <td>한국타이어앤테크놀로지(주)</td>\n",
              "      <td>37172.367188</td>\n",
              "      <td>37150.0</td>\n",
              "      <td>22.367188</td>\n",
              "      <td>한국타이어앤테크놀로지(주)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11129</th>\n",
              "      <td>116215.335938</td>\n",
              "      <td>116200.0</td>\n",
              "      <td>15.335938</td>\n",
              "      <td>(주)오리온</td>\n",
              "      <td>116215.335938</td>\n",
              "      <td>116200.0</td>\n",
              "      <td>15.335938</td>\n",
              "      <td>(주)오리온</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11130</th>\n",
              "      <td>109008.218750</td>\n",
              "      <td>109000.0</td>\n",
              "      <td>8.218750</td>\n",
              "      <td>롯데웰푸드(주)</td>\n",
              "      <td>109008.218750</td>\n",
              "      <td>109000.0</td>\n",
              "      <td>8.218750</td>\n",
              "      <td>롯데웰푸드(주)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11131</th>\n",
              "      <td>92616.812500</td>\n",
              "      <td>92600.0</td>\n",
              "      <td>16.812500</td>\n",
              "      <td>(주)종근당</td>\n",
              "      <td>92616.812500</td>\n",
              "      <td>92600.0</td>\n",
              "      <td>16.812500</td>\n",
              "      <td>(주)종근당</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1113200 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d4f2fda-ab8f-4ec5-b4ad-7e4e498de7c3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d4f2fda-ab8f-4ec5-b4ad-7e4e498de7c3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d4f2fda-ab8f-4ec5-b4ad-7e4e498de7c3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-554303a2-27b5-4cc2-be47-0fdc2961f99c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-554303a2-27b5-4cc2-be47-0fdc2961f99c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-554303a2-27b5-4cc2-be47-0fdc2961f99c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result"
      ],
      "metadata": {
        "id": "sHOGZm0cXV20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_data=pd.DataFrame()\n",
        "i=0\n",
        "for i in range(0,1113200,44):\n",
        "    df=whole_data[i:i+44][['top_predict_price','top_real_price','top_difference','top_five_name']]\n",
        "    df=df.sort_values('top_difference')\n",
        "    top1=df.head(1)\n",
        "    bottom1=df.tail(1)\n",
        "    top_bottom=pd.concat([top1, bottom1], axis=1,ignore_index=True)\n",
        "    final_data=pd.concat([top_bottom, final_data], ignore_index=True)\n",
        "    i=i+0\n",
        "\n",
        "print('<top>')\n",
        "print(final_data[3].value_counts())\n",
        "print('*'*30)\n",
        "print('<bottom>')\n",
        "final_data[7].value_counts()"
      ],
      "metadata": {
        "id": "aDNm9FVZB07f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e4ebd3-ed1a-477a-f011-405240c2a973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<top>\n",
            "롯데웰푸드(주)          1104\n",
            "현대글로비스(주)          993\n",
            "삼성에스디에스(주)         986\n",
            "(주)이마트             953\n",
            "현대자동차(주)           889\n",
            "(주)셀트리온            852\n",
            "(주)녹십자             847\n",
            "(주)아모레퍼시픽          806\n",
            "(주)종근당             790\n",
            "금호석유화학(주)          784\n",
            "(주)오리온             758\n",
            "에스케이하이닉스(주)        659\n",
            "엘지이노텍(주)           658\n",
            "(주)한솔케미칼           646\n",
            "현대모비스(주)           616\n",
            "(주)제일기획            605\n",
            "(주)케이티앤지           595\n",
            "삼양식품(주)            567\n",
            "(주)현대백화점           565\n",
            "(주)에스원             540\n",
            "(주)DB하이텍           528\n",
            "(주)신세계             521\n",
            "(주)한섬              509\n",
            "(주)현대홈쇼핑           484\n",
            "(주)LG유플러스          477\n",
            "(주)케이티             475\n",
            "고려아연(주)            472\n",
            "(주)영원무역            463\n",
            "(주)녹십자홀딩스          462\n",
            "피아이첨단소재(주)         450\n",
            "(주)한화              441\n",
            "한전KPS(주)           422\n",
            "티케이지휴켐스(주)         421\n",
            "롯데정밀화학(주)          416\n",
            "(주)동서              409\n",
            "동원시스템즈(주)          398\n",
            "한국타이어앤테크놀로지(주)     397\n",
            "삼성전자(주)            390\n",
            "(주)지에스리테일          362\n",
            "(주)오뚜기             357\n",
            "(주)엔씨소프트           334\n",
            "(주)농심              329\n",
            "(주)LG생활건강          289\n",
            "기아(주)              281\n",
            "Name: 3, dtype: int64\n",
            "******************************\n",
            "<bottom>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(주)LG생활건강         6244\n",
              "(주)엔씨소프트          1051\n",
              "(주)신세계             931\n",
              "고려아연(주)            874\n",
              "삼성에스디에스(주)         799\n",
              "(주)한솔케미칼           754\n",
              "(주)오뚜기             712\n",
              "(주)이마트             630\n",
              "(주)제일기획            618\n",
              "(주)농심              613\n",
              "(주)현대백화점           597\n",
              "롯데웰푸드(주)           591\n",
              "삼양식품(주)            567\n",
              "(주)한화              563\n",
              "(주)종근당             552\n",
              "엘지이노텍(주)           540\n",
              "(주)셀트리온            510\n",
              "(주)녹십자             488\n",
              "(주)케이티             459\n",
              "(주)오리온             448\n",
              "(주)녹십자홀딩스          426\n",
              "(주)현대홈쇼핑           423\n",
              "금호석유화학(주)          420\n",
              "(주)아모레퍼시픽          414\n",
              "한국타이어앤테크놀로지(주)     410\n",
              "(주)케이티앤지           395\n",
              "(주)지에스리테일          374\n",
              "에스케이하이닉스(주)        359\n",
              "현대자동차(주)           336\n",
              "(주)동서              322\n",
              "현대글로비스(주)          311\n",
              "삼성전자(주)            290\n",
              "(주)한섬              288\n",
              "(주)LG유플러스          273\n",
              "피아이첨단소재(주)         261\n",
              "(주)영원무역            237\n",
              "동원시스템즈(주)          226\n",
              "롯데정밀화학(주)          205\n",
              "(주)에스원             193\n",
              "(주)DB하이텍           171\n",
              "현대모비스(주)           170\n",
              "한전KPS(주)           126\n",
              "기아(주)               67\n",
              "티케이지휴켐스(주)          62\n",
              "Name: 7, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top=pd.DataFrame(whole_data['top_five_name'].value_counts())\n",
        "# top.head(30)"
      ],
      "metadata": {
        "id": "kiZTTnBBsBEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bottom=pd.DataFrame(whole_data['bottom_five_name'].value_counts())\n",
        "# bottom.head(30)"
      ],
      "metadata": {
        "id": "eBitP0lJsQMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bottom.shape"
      ],
      "metadata": {
        "id": "p8ZGSMpRwkbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end=time.time()"
      ],
      "metadata": {
        "id": "Sh9gcemwPJg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Time:', np.round((end-start)/3600,2),'H') ## GPU 사용됨 ##"
      ],
      "metadata": {
        "id": "rYhn_9oAPMVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360f5928-fc83-401e-b851-ddf6a11d4b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 0.2 H\n"
          ]
        }
      ]
    }
  ]
}